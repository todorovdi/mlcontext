{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b4c80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%debug\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from os.path import join as pjoin\n",
    "\n",
    "import os,sys; sys.path.append(os.path.expandvars('$CODE_MEMORY_ERRORS'))\n",
    "#%run -i ../exec_HPC.py 0\n",
    "from config2 import path_code\n",
    "from IPython import get_ipython; ipython = get_ipython()\n",
    "\n",
    "locs = {}\n",
    "\n",
    "from config2 import subjects\n",
    "\n",
    "import numpy as np\n",
    "from os.path import join as pjoin\n",
    "dir_rr = '/home/demitau/ownCloud/Current/merr_data/Romain_calc_res/'\n",
    "an = 'prevfeedback_preverrors_errors_prevbelief'\n",
    "random = pjoin(dir_rr, f'sub01_WGPOZPEE_randompartial_scores_{an}_broad.npy')\n",
    "stable = pjoin(dir_rr, f'sub01_WGPOZPEE_stablepartial_scores_{an}_broad.npy')\n",
    "scores_romain_b2b = {}\n",
    "\n",
    "scores_romain_b2b['stable'] = np.load(stable)# 5 splits each\n",
    "scores_romain_b2b['random'] = np.load(random)\n",
    "\n",
    "display(scores_romain_b2b)\n",
    "print(an.split('_'))\n",
    "# time_lock = target,  control = feedback    ( I had control = movement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cee9676",
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject in subjects[:1]:\n",
    "#for subject in subjects[1:]:\n",
    "    print('   ', subject)\n",
    "    n_jobs = 10\n",
    "    hpass = 'no_filter'\n",
    "    regression_type = 'Ridge'\n",
    "    ipython.run_line_magic('run',\n",
    "        f' -i {path_code}/spoc_home_with_prev_error2_orig.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79cd51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = np.array(behav_df['target_inds'])\n",
    "targets_stable = targets[stable]\n",
    "targets_random = targets[random]\n",
    "# Feedback positions\n",
    "feedback = np.array(behav_df['feedback'])\n",
    "feedback_stable = feedback[stable]\n",
    "feedback_random = feedback[random]\n",
    "feedbackX = np.array(behav_df['feedbackX'])\n",
    "feedbackX_stable = feedbackX[stable]\n",
    "feedbackX_random = feedbackX[random]\n",
    "feedbackY = np.array(behav_df['feedbackY'])\n",
    "feedbackY_stable = feedbackY[stable]\n",
    "feedbackY_random = feedbackY[random]\n",
    "# Movement positions\n",
    "movement = np.array(behav_df['org_feedback'])\n",
    "movement_stable = movement[stable]\n",
    "movement_random = movement[random]\n",
    "# Error positions\n",
    "errors = np.array(behav_df['error'])\n",
    "errors_stable = errors[stable]\n",
    "errors_random = errors[random]\n",
    "abs_errors_stable = np.abs(errors_stable)\n",
    "abs_errors_random = np.abs(errors_random)\n",
    "# Belief\n",
    "belief = np.array(behav_df['belief'])\n",
    "belief_stable = belief[stable]\n",
    "belief_random = belief[random]\n",
    "# keep only non_hit trials\n",
    "non_hit_stable = point_in_circle(targets_stable, target_coords,\n",
    "                                 feedbackX_stable, feedbackY_stable,\n",
    "                                 radius_target + radius_cursor)\n",
    "non_hit_random = point_in_circle(targets_random, target_coords,\n",
    "                                 feedbackX_random, feedbackY_random,\n",
    "                                 radius_target + radius_cursor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6a32bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a3d4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_scores_many = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ca3628",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = epochs.times\n",
    "if control == 'feedback':\n",
    "    analysis_name = 'prevfeedback_preverrors_errors_prevbelief'\n",
    "elif control == 'movement':\n",
    "    analysis_name = 'prevmovement_preverrors_errors_prevbelief'\n",
    "\n",
    "# Regression for classic decoding\n",
    "alphas = np.logspace(-5, 5, 12)\n",
    "est = make_pipeline(SPoC(n_components=5, log=True, reg='oas',\n",
    "                         rank='full'),\n",
    "                    RidgeCV(alphas=alphas))\n",
    "# Regressions for the B2B\n",
    "G = make_pipeline(SPoC(n_components=5, log=True, reg='oas',\n",
    "                       rank='full'),\n",
    "                  RidgeCV(alphas=alphas, fit_intercept=False))\n",
    "H = LinearRegression(fit_intercept=False)\n",
    "if DEBUG:\n",
    "    b2b = B2B_SPoC(G=G, H=H, n_splits=60)\n",
    "else:\n",
    "    #b2b = B2B_SPoC(G=G, H=H, n_splits=30)\n",
    "\n",
    "    ## NEW\n",
    "    n_splits = 200\n",
    "    b2b = B2B_SPoC(G=G, H=H, n_splits=n_splits, parallel_type='across_splits',\n",
    "               n_jobs=10)\n",
    "\n",
    "\n",
    "# Cross-validation\n",
    "cv = KFold(nb_fold, shuffle=True)\n",
    "\n",
    "#  Run decoding\n",
    "envs = ['stable', 'stable', 'stable', 'stable']\n",
    "#envs = ['stable', 'random']\n",
    "if DEBUG:\n",
    "    if isinstance(env_to_run,list):\n",
    "        envs = env_to_run\n",
    "    else:\n",
    "        envs = [env_to_run]\n",
    "#for env in ['stable', 'random']:\n",
    "#for env in ['stable', 'random'][::-1]:\n",
    "for env in envs:\n",
    "    if env == 'stable':\n",
    "        ep = epochs['20', '21', '22', '23', '30']\n",
    "        prev_errors_stable = np.insert(errors_stable, 0, 0)[:-1]\n",
    "        prev_targets_stable = np.insert(targets_stable, 0, 0)[:-1]\n",
    "        prev_movement_stable = np.insert(movement_stable, 0, 0)[:-1]\n",
    "        prev_feedback_stable = np.insert(feedback_stable, 0, 0)[:-1]\n",
    "        prev_belief_stable = np.insert(belief_stable, 0, 0)[:-1]\n",
    "        if control == 'feedback':\n",
    "            analysis_value = [prev_feedback_stable, prev_errors_stable,\n",
    "                              errors_stable, prev_belief_stable]\n",
    "        elif control == 'movement':\n",
    "            analysis_value = [prev_movement_stable, prev_errors_stable,\n",
    "                              errors_stable, prev_belief_stable]\n",
    "        non_hit = non_hit_stable\n",
    "        non_hit = np.array(non_hit)\n",
    "        # remove trials following hit (because no previous error)\n",
    "        non_hit = ~(~non_hit | ~np.insert(non_hit, 0, 1)[:-1])\n",
    "        non_hit[[0, 192]] = False  # Removing first trial of each block\n",
    "    elif env == 'random':\n",
    "        ep = epochs['25', '26', '27', '28', '35']\n",
    "        prev_errors_random = np.insert(errors_random, 0, 0)[:-1]\n",
    "        prev_targets_random = np.insert(targets_random, 0, 0)[:-1]\n",
    "        prev_movement_random = np.insert(movement_random, 0, 0)[:-1]\n",
    "        prev_feedback_random = np.insert(feedback_random, 0, 0)[:-1]\n",
    "        prev_belief_random = np.insert(belief_random, 0, 0)[:-1]\n",
    "        if control == 'feedback':\n",
    "            analysis_value = [prev_feedback_random, prev_errors_random,\n",
    "                              errors_random, prev_belief_random]\n",
    "        elif control == 'movement':\n",
    "            analysis_value = [prev_movement_random, prev_errors_random,\n",
    "                              errors_random, prev_belief_random]\n",
    "        non_hit = non_hit_random\n",
    "        non_hit = np.array(non_hit)\n",
    "        # remove trials following hit (because no previous error)\n",
    "        non_hit = ~(~non_hit | ~np.insert(non_hit, 0, 1)[:-1])\n",
    "        non_hit[[0, 192]] = False  # Removing first trial of each block\n",
    "    X = ep.pick_types(meg=True, ref_meg=False)._data\n",
    "    #wh = (times > -0.45) & (times < -0.05)\n",
    "    # NEW\n",
    "    wh = (times > -0.45) & (times < 0.05)\n",
    "    X = X[non_hit]\n",
    "    X = X[:, :, wh]\n",
    "    Y = np.array(analysis_value)\n",
    "    Y = Y[:, non_hit].T\n",
    "\n",
    "    ######################  DEBUG to check var values\n",
    "    if DEBUG:\n",
    "        from config2 import DEBUG_ntrials,DEBUG_nchannels\n",
    "        X = X[:DEBUG_ntrials,:DEBUG_nchannels]\n",
    "        Y = Y[:DEBUG_ntrials]\n",
    "    ######################  DEBUG to check var values\n",
    "        #sys.exit(0)\n",
    "\n",
    "    # Classic decoding\n",
    "    dd = 0\n",
    "    if dd:\n",
    "        scoring = make_scorer(scorer_spearman)\n",
    "        scores = list()\n",
    "        for ii in range(Y.shape[1]):\n",
    "            y = Y[:, ii]\n",
    "            with mne.use_log_level('warning'):\n",
    "                score = cross_val_multiscore(est, X, y=y, cv=cv, scoring=scoring,\n",
    "                                            n_jobs=n_jobs)\n",
    "            score = score.mean(axis=0)\n",
    "            scores.append(score)\n",
    "        scores = np.array(scores)\n",
    "\n",
    "        print(env, scores)\n",
    "    \n",
    "    ######################  DEBUG to check var values\n",
    "    #sys.exit(0)\n",
    "\n",
    "\n",
    "    # run partial decoding\n",
    "    Y = scale(Y)\n",
    "    with mne.use_log_level('warning'):\n",
    "        b2b.fit(X, Y)\n",
    "    partial_scores = np.diag(b2b.E_)\n",
    "    \n",
    "    partial_scores_many += [partial_scores]\n",
    "    \n",
    "    print(n_splits, env, partial_scores)\n",
    "    # save scores\n",
    "    fname = op.join(results_folder,\n",
    "                    '%s_%sscores_%s_%s.npy' % (subject, env,\n",
    "                                               analysis_name, freq_name))\n",
    "    np.save(fname, np.array(scores))\n",
    "    fname = op.join(results_folder,\n",
    "                    '%s_%spartial_scores_%s_%s.npy' % (subject, env,\n",
    "                                                       analysis_name,\n",
    "                                                       freq_name))\n",
    "    np.save(fname, np.array(partial_scores))\n",
    "    \n",
    "    print('Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e68a704",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = ICA, hpass, freq_name,  control, time_locked,   env\n",
    "pss = ';'.join( list( map(str,ps) ) )\n",
    "print(pss)\n",
    "fnf = op.join(results_folder, f'{pss}_XY.npz'); \n",
    "print(fnf)\n",
    "np.savez_compressed(fnf, X=X,Y=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7a27b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439a3f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9334f80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fb2ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d773bf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "#prep = f'/home/demitau/data_Quentin/full_experiments/data2/sub01_WGPOZPEE/results/spoc_home2_{hpass}_almost_orig/'\n",
    "prep = f'/home/demitau/data_Quentin/full_experiments/data2/sub01_WGPOZPEE/results/spoc_home2_{hpass}_orig/'\n",
    "for env in ['stable','random']:\n",
    "    p = prep + f'{subject}_{env}partial_scores_prevmovement_preverrors_errors_prevbelief_broad.npy'\n",
    "    dt = datetime.fromtimestamp( os.stat(p).st_mtime )\n",
    "    r = np.load(p)\n",
    "    #print(env, r.mean(), r.std() )\n",
    "    print( env, list( map( lambda x: f'{x:.5f}', r) ), dt )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04da8f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "stable ['-0.00514', '0.03931', '0.03942', '0.05747'] 2023-04-18 12:23:00.949813\n",
    "random ['0.00009', '-0.00360', '-0.01051', '0.00283'] 2023-04-18 12:24:57.069810"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9dc85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31697c4f",
   "metadata": {},
   "source": [
    "# compare spoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb18ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = 'stable'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718d8b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from base2_orig import B2B_SPoC as B2B_SPoC_old\n",
    "from base2 import  B2B_SPoC\n",
    "\n",
    "from mne.decoding import (cross_val_multiscore, LinearModel,\n",
    "                          SPoC)\n",
    "\n",
    "G = make_pipeline(SPoC(n_components=5, log=True, reg='oas',\n",
    "                       rank='full'),\n",
    "                  RidgeCV(alphas=alphas, fit_intercept=False))\n",
    "H = LinearRegression(fit_intercept=False)\n",
    "\n",
    "n_splits = 20\n",
    "ensemble = ShuffleSplit(n_splits=n_splits, \n",
    "        test_size=.5, random_state = 0)\n",
    "\n",
    "# TODO many runs with diff random states to check\n",
    "print(env)\n",
    "\n",
    "b2b_type2b2b = {}\n",
    "\n",
    "pt = 'across_splits'\n",
    "b2b = B2B_SPoC(G=G, H=H, n_splits=n_splits, parallel_type=pt,\n",
    "           n_jobs=10, ensemble = ensemble)\n",
    "b2b_type2b2b[f'parallel_{pt}'] = b2b\n",
    "\n",
    "pt = 'across_splits_and_dims'\n",
    "b2b = B2B_SPoC(G=G, H=H, n_splits=n_splits, parallel_type=pt,\n",
    "           n_jobs=10, ensemble = ensemble)\n",
    "b2b_type2b2b[f'parallel_{pt}'] = b2b\n",
    "\n",
    "pt = 'no'\n",
    "b2b = B2B_SPoC(G=G, H=H, n_splits=n_splits, parallel_type=pt,\n",
    "           n_jobs=10, ensemble = ensemble)\n",
    "b2b_type2b2b[f'parallel_{pt}'] = b2b\n",
    "\n",
    "# b2b = B2B_SPoC_old(G=G, H=H, n_splits=n_splits,\n",
    "#                    ensemble = ensemble)\n",
    "# b2b_type2b2b[f'old'] = b2b\n",
    "\n",
    "b2b_type2res = {}\n",
    "for b2b_type, b2b  in b2b_type2b2b.items():\n",
    "    with mne.use_log_level('warning'):\n",
    "        b2b.fit(X, Y)\n",
    "    partial_scores = np.diag(b2b.E_)\n",
    "    print(b2b_type, partial_scores)\n",
    "    b2b_type2res[b2b_type] = partial_scores\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c8efcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "b2b_type2b2b.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a4a0f2",
   "metadata": {},
   "source": [
    "# debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb345575",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "fnf = '/home/demitau/data_Quentin/full_experiments/data2/sub01_WGPOZPEE/results/spoc_home2_no_filter_orig/with_ICA;no_filter;broad;movement;target;stable_XY.npz'\n",
    "f = np.load(fnf)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "X = f['X']\n",
    "Y = f['Y']\n",
    "print( X.shape)  # trial x sensors x time \n",
    "print( Y.shape)  # trial x  dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92af4d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "Y_test = np.array([[-0.35002536],\n",
    "       [-1.48563779],\n",
    "       [ 1.24180457],\n",
    "       [-1.13094076]])\n",
    "\n",
    "X_test = np.array([[[-2.11474666e-14,  6.48929128e-15]],\n",
    "\n",
    "       [[-7.59599106e-14, -1.82705380e-14]],\n",
    "\n",
    "       [[ 9.82457510e-14,  6.47432584e-14]],\n",
    "\n",
    "       [[ 2.54159085e-14,  3.92063100e-14]]])\n",
    "\n",
    "n_splits = 2\n",
    "ensemble = ShuffleSplit(n_splits=n_splits, \n",
    "        test_size=.5, random_state = 0)\n",
    "list(ensemble.split(X_test,Y_test))\n",
    "print('s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18b54d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nt = 6  # cannot have less\n",
    "X_test = X[:nt, :2, :2]\n",
    "Y_test = Y[:nt, :2]\n",
    "\n",
    "print( X_test.shape)\n",
    "print( Y_test.shape)\n",
    "\n",
    "n_splits = 3\n",
    "ensemble = ShuffleSplit(n_splits=n_splits, \n",
    "        test_size=.5, random_state = 0)\n",
    "list(ensemble.split(X,Y))\n",
    "print('s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bd3822",
   "metadata": {
    "code_folding": [
     10,
     18
    ]
   },
   "outputs": [],
   "source": [
    "# import importlib as il\n",
    "# import base2\n",
    "# il.reload(base2)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from os.path import join as pjoin\n",
    "\n",
    "import os,sys; sys.path.append(os.path.expandvars('$CODE_MEMORY_ERRORS'))\n",
    "\n",
    "from mne.decoding import (cross_val_multiscore, LinearModel,\n",
    "                          SPoC)\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import Ridge, RidgeCV, LinearRegression\n",
    "\n",
    "import mne\n",
    "alphas = np.logspace(-5, 5, 12)\n",
    "\n",
    "G = make_pipeline(SPoC(n_components=5, log=True, reg='oas',\n",
    "                       rank='full'),\n",
    "                  RidgeCV(alphas=alphas, fit_intercept=False))\n",
    "H = LinearRegression(fit_intercept=False)\n",
    "\n",
    "from base2_orig import B2B_SPoC as B2B_SPoC_old\n",
    "from base2 import  B2B_SPoC\n",
    "\n",
    "n_jobs_test = 2\n",
    "\n",
    "\n",
    "b2b_type2b2b = {}\n",
    "\n",
    "pt = 'across_splits_and_dims'\n",
    "b2b = B2B_SPoC(G=G, H=H, n_splits=n_splits, parallel_type=pt,\n",
    "           n_jobs=n_jobs_test, ensemble = ensemble)\n",
    "b2b_type2b2b[f'parallel_{pt}'] = b2b\n",
    "\n",
    "pt = 'across_splits'\n",
    "b2b = B2B_SPoC(G=G, H=H, n_splits=n_splits, parallel_type=pt,\n",
    "           n_jobs=n_jobs_test, ensemble = ensemble)\n",
    "b2b_type2b2b[f'parallel_{pt}'] = b2b\n",
    "\n",
    "# b2b = B2B_SPoC_old(G=G, H=H, n_splits=n_splits,\n",
    "#                    ensemble = ensemble)\n",
    "# b2b_type2b2b[f'old'] = b2b\n",
    "\n",
    "# H_sets are the same\n",
    "\n",
    "b2b_type2res = {}\n",
    "for b2b_type, b2b  in b2b_type2b2b.items():\n",
    "    with mne.use_log_level('warning'):\n",
    "        b2b.fit(X_test, Y_test)\n",
    "    partial_scores = np.diag(b2b.E_)\n",
    "    print('--- res ', b2b_type, partial_scores)\n",
    "    b2b_type2res[b2b_type] = partial_scores\n",
    "\n",
    "d = b2b_type2res['parallel_across_splits'] - b2b_type2res['parallel_across_splits_and_dims']\n",
    "print('diff = ', np.max(np.abs(d)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63e6d22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba26379",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a918914",
   "metadata": {},
   "outputs": [],
   "source": [
    "mne.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0381d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0ca3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c847c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_jobs_test = 10\n",
    "\n",
    "nt = 150  \n",
    "ndim = Y.shape[1]\n",
    "nsens = 150\n",
    "ntimes = 80\n",
    "n_splits = 10\n",
    "\n",
    "# nt = 1500  \n",
    "# ndim = Y.shape[1]\n",
    "# nsens = 1500\n",
    "# ntimes = 8000\n",
    "#n_splits = 30\n",
    "\n",
    "# nt = 15\n",
    "# ndim = Y.shape[1] \n",
    "# nsens = 30\n",
    "# ntimes = 20\n",
    "X_test = X[:nt, :nsens, :ntimes]\n",
    "Y_test = Y[:nt, :ndim]\n",
    "#X_test = X[:nt, :1, :2]  -- works\n",
    "#Y_test = Y[:nt, :1]\n",
    "\n",
    "\n",
    "print( X_test.shape)\n",
    "print( Y_test.shape)\n",
    "\n",
    "\n",
    "ensemble = ShuffleSplit(n_splits=n_splits, \n",
    "        test_size=.5, random_state = 0)\n",
    "list(ensemble.split(X,Y))\n",
    "\n",
    "b2b_type2b2b = {}\n",
    "\n",
    "pt = 'across_splits'\n",
    "b2b = B2B_SPoC(G=G, H=H, n_splits=n_splits, parallel_type=pt,\n",
    "           n_jobs=n_jobs_test, ensemble = ensemble)\n",
    "b2b_type2b2b[f'parallel_{pt}'] = b2b\n",
    "\n",
    "pt = 'across_splits_and_dims'\n",
    "b2b = B2B_SPoC(G=G, H=H, n_splits=n_splits, parallel_type=pt,\n",
    "           n_jobs=n_jobs_test, ensemble = ensemble)\n",
    "b2b_type2b2b[f'parallel_{pt}'] = b2b\n",
    "\n",
    "b2b_as = b2b_type2b2b['parallel_across_splits']\n",
    "b2b_asd = b2b_type2b2b['parallel_across_splits_and_dims']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb0cda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "b2b_type2res = {}\n",
    "for b2b_type, b2b  in b2b_type2b2b.items():\n",
    "    print('s', time.time())\n",
    "    with mne.use_log_level('warning'):\n",
    "        b2b.fit(X_test, Y_test)\n",
    "    partial_scores = np.diag(b2b.E_)\n",
    "    print('--- res ', b2b_type, partial_scores)\n",
    "    print('--------------------------------')\n",
    "    print('--------------------------------')\n",
    "    b2b_type2res[b2b_type] = partial_scores\n",
    "    \n",
    "    print('e', time.time())\n",
    "    \n",
    "d = b2b_type2res['parallel_across_splits'] - b2b_type2res['parallel_across_splits_and_dims']\n",
    "print('diff = ', np.max(np.abs(d)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f00f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "b2b_asd.fit(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700d9b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "b2b_as.fit(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e638867e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969f44fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5318a25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174849c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import _pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1096f76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "b2b_type2res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbac07b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01229f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2958288a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y hat for only split has shape 4,2 for    otehr -- 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c767e513",
   "metadata": {},
   "outputs": [],
   "source": [
    "for b2b_type, b2b  in b2b_type2b2b.items():\n",
    "    H_hats = b2b.H_hats\n",
    "    print(b2b_type, H_hats.shape)\n",
    "    display(H_hats)\n",
    "    display(H_hats.reshape((3,2,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37614b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7f6ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from base2 import _b2b_fit_one_split\n",
    "_b2b_fit_one_split(0,X_test,Y_test, [0], [1], G,H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3c0058",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
