{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da58d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "# Get the list of first level subdirectories in dir\n",
    "data_dir = os.path.expanduser('~/data_Quentin/full_experiments/context_change_MEG')\n",
    "subdirs = [os.path.join(data_dir, d) for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))]\n",
    "\n",
    "# Initialize an empty dictionary\n",
    "result = {}\n",
    "\n",
    "# Loop through each subdirectory\n",
    "fnbs = []\n",
    "for subdir in subdirs:\n",
    "    # Get the path of the behav directory\n",
    "    behav_dir = os.path.join(subdir, \"behav\")\n",
    "    # Get the list of log files in the behav directory\n",
    "    log_files = [os.path.join(behav_dir, f) for f in os.listdir(behav_dir) if f.endswith(\".log\")]\n",
    "    # Find the largest log file by size\n",
    "    largest_log = max(log_files, key=os.path.getsize)\n",
    "    # Get the name of the first level subdirectory\n",
    "    subdir_name = os.path.basename(subdir)\n",
    "    # Create a sub-dictionary with the logpath and param_path keys and values\n",
    "    subdict = {\"log_path\": largest_log, \"param_path\": largest_log.replace(\".log\", \".param\"),\n",
    "              'log_size_mb':os.path.getsize(largest_log) / (1024**2)}\n",
    "    fnbs.append(largest_log.replace(\".log\", ''))\n",
    "    # Add the sub-dictionary to the result dictionary with the subdir_name as the key\n",
    "    result[subdir_name] = subdict\n",
    "\n",
    "# Print the result dictionary\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85caed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fnbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0ee103",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_behav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0906af8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51f586f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774e32cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys; sys.path.append(os.path.expandvars('$CODE_MEMORY_ERRORS'))\n",
    "from datetime import datetime\n",
    "from behav_proc import readParamFiles, addBasicInfo, getGeomInfo\n",
    "from behav_proc import loadTriggerLog\n",
    "import pandas as pd\n",
    "from os.path import join as pjoin\n",
    "\n",
    "dfs_unproc = []\n",
    "dftriglogs = []\n",
    "for fnfb in fnbs:\n",
    "    path_behav = os.path.dirname(fnfb)\n",
    "    fnb = os.path.basename(fnfb)\n",
    "    print(fnb)\n",
    "    fn = fnb + '.log'\n",
    "    fnp = fnb + '.param'\n",
    "    \n",
    "    params, phase2trigger, trigger2phase, CONTEXT_TRIGGER_DICT  = \\\n",
    "        readParamFiles(fnp, path_behav)\n",
    "    \n",
    "    #fnf = fnb + '.log'\n",
    "    #fnp = fnb + '.param'\n",
    "\n",
    "    with open(pjoin(path_behav,fn), 'r') as f:\n",
    "         l = f.readline()\n",
    "         truelen = len( l.split(',') )\n",
    "\n",
    "    r = ('trial_index, current_phase_trigger, tgti_to_show,'\n",
    "       ' vis_feedback_type, trial_type, special_block_type, block_ind, '\n",
    "        ' feedbackX, feedbackY, unpert_feedbackX, unpert_feedbackY,'\n",
    "         ' error_distance, target_coordX, target_coordY, '\n",
    "         'feedbackX_when_crossing, feedbackY_when_crossing, '\n",
    "         'jax1, jax2, reward, time, time_abs')\n",
    "    r = r.replace(' ','')\n",
    "    colnames = r.split(',')\n",
    "    print('len(colnames) = ',len(colnames),'truelen = ',truelen,colnames)\n",
    "    assert truelen == len(colnames)\n",
    "    \n",
    "    dtypes = (int,int,str,str,str,str,int,float,float,float,float,\n",
    "        float,float, float,float,float,float, float,float,float,\n",
    "         float)\n",
    "    ddt = dict(zip(colnames,dtypes))\n",
    "    ddt['target_coordX'] = str\n",
    "    ddt['target_coordY'] = str\n",
    "\n",
    "    nbad = 0  # recompense strip, but in first particiapnt no\n",
    "    #colnames = ['trial_index', 'current_phase_']\n",
    "    df = pd.read_csv(pjoin(path_behav,fn), skipfooter = nbad, \n",
    "                     on_bad_lines='warn', header=0,\n",
    "                    names=colnames, encoding='latin1',\n",
    "                    comment='#', dtype=ddt)\n",
    "    #encoding='latin-1'\n",
    "    subj_ = fnb.split('_')[0]    \n",
    "    df['subject'] = subj_\n",
    "    #dfs += [df]\n",
    "    \n",
    "    df['phase'] = df.apply(lambda row: trigger2phase[row['current_phase_trigger']], 1)\n",
    "    \n",
    "    fnf = pjoin(path_behav, fnb + '_trigger.log')\n",
    "    dftriglog = loadTriggerLog(fnf,CONTEXT_TRIGGER_DICT)\n",
    "    dftriglog['subject'] = subj_\n",
    "    dftriglogs += [dftriglog]\n",
    "    \n",
    "    ma = df['time_abs'].max()\n",
    "    ma = datetime.fromtimestamp(ma).strftime(\"%H:%M:%S\")\n",
    "    mi = df['time_abs'].min()\n",
    "    mi = datetime.fromtimestamp(mi).strftime(\"%H:%M:%S\")\n",
    "    print(mi,'   ', ma)\n",
    "    \n",
    "    # repair bad target inds for phases where target should be\n",
    "    # meaningful\n",
    "    bad_tgti_inds = df.loc[(df['tgti_to_show'].\\\n",
    "            isin(['None','-1'] ) ) &\\\n",
    "            (df['phase'] == 'GO_CUE_WAIT_AND_SHOW')].index\n",
    "    print('len(bad_tgti_inds) =',len(bad_tgti_inds))\n",
    "    for ind in bad_tgti_inds:\n",
    "        nextind = df.loc[ind+1, 'tgti_to_show']\n",
    "        assert nextind in ['0','1','2']\n",
    "        print( df.loc[ind+1, 'tgti_to_show'] )\n",
    "        df.loc[ind,'tgti_to_show'] = nextind\n",
    "        \n",
    "    bad_tgti_inds2 = df.loc[(df['tgti_to_show'].isin(['None','-1'] ) ) ].index\n",
    "    print('len(bad_tgti_inds2) =',len(bad_tgti_inds2))\n",
    "    df.loc[bad_tgti_inds2,'tgti_to_show'] = '-10000'\n",
    "    df.loc[bad_tgti_inds2,'target_coordX'] = '-10000'\n",
    "    df.loc[bad_tgti_inds2,'target_coordY'] = '-10000'\n",
    "    \n",
    "    df['tgti_to_show'] = df['tgti_to_show'].astype(int)\n",
    "    #df['target_coordX'] = df['target_coordX'].astype(int)\n",
    "    #df['target_coordY'] = df['target_coordY'].astype(int)\n",
    "    \n",
    "    df_unproc = df\n",
    "    dfs_unproc.append(df_unproc)\n",
    "\n",
    "print( 'len = {}, maxtime in min = {}'.format( len(df), df['time'].max() / 60 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f353862",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfall_unproc = pd.concat(dfs_unproc)\n",
    "dfall_triglog = pd.concat(dftriglogs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbd1016",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirout = '~/current/merr_data/'\n",
    "dfall_unproc.to_pickle(os.path.join(dirout, \"ccMEG_bahav_collected_rowframe.pkl.gz\") ,compression='zip')\n",
    "# Save the result to a compressed pickle file in dirout\n",
    "#with gzip.open(os.path.join(dirout, \"ccMEG_bahav_collected.pkl.gz\"), \"wb\") as f:\n",
    "#    pickle.dump(result, f)\n",
    "dfall_triglog.to_pickle(os.path.join(dirout, \"ccMEG_bahav_collected_rowtrig.pkl.gz\") ,compression='zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4267505a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "merr",
   "language": "python",
   "name": "merr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
